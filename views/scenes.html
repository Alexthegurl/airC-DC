<!doctype html>
<html lang="en">
<head>
  <title>AirC&#9889DC</title>
  <!-- http://www.webrtc.org/ -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
   <link rel="stylesheet" type="text/css" href="/css/style.css">
  
</head>
<body id="scenes">

<table align="center">
  <tr>
    <th><a href="/bass"><button class="button">Air Bass</button></a></th>
    <th><a href="/drums"><button class="button">Air Drums</button></a></th>
    <th><a href="/dj1"><button class="button">Air DJ Set 1</button></a></th>
    <th><a href="/dj2"><button class="button">Air DJ Set2</button></a></th>
    <th><a href="/guitar"><button class="button">Air Guitar</button></a></th>
    <th><a href="/piano"><button class="button">Air Piano</button></a></th>
    <th><a href="/gong"><button class="button">Hit The Gong!!!</button></a></th>
    <th><a href="/team"><button class="button">About the Team</button></a></th>
    <th><a href="/trump"><button class="button">Trump Mode</button></a></th>
    <th><a href="/"><button class="button">Home</button></a></th>

  </tr>
</table>

<video id="monitor" autoplay  width: 500; height: 500; style="margin: auto; top: 150px;"></video><br>

<!-- video canvas is the mirror -->
  <canvas id="videoCanvas" width="500px" height="500px"; style="margin: auto; top: 132px;"></canvas><br>

  <!-- layer 2 has the button/instrument detections in them -->
  <canvas id="layer2" width="500px" height="500px" style="margin: auto; top: 150px;"></canvas><br>


<!-- blend canvas is where all the photos are being taken in black and white and comparing the pixel changes based on diff. background image, detecting "movement" when the images are different from different motions -->
<canvas id="blendCanvas" width="500px" height="500px" style="margin: auto; position: relative; bottom: 375px; left: 550px;" ></canvas>

<canvas id="greenCanvas" width="500px" height="500px" style="margin: auto; position: relative; bottom: 375px; left: 550px;" ></canvas>
<div id="messageError"></div>
<div id="messageArea" style="position: absolute; left: 25px; top: 150px;" align="center"></div>

<!-- this script is what launches the audio and webcam -->
<script>
navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
window.URL = window.URL || window.webkitURL;

var camvideo = document.getElementById('monitor');

  if (!navigator.getUserMedia) 
  {
    document.getElementById('messageError').innerHTML = 
      'Sorry. <code>navigator.getUserMedia()</code> is not available.';
  }
  navigator.getUserMedia({video: true}, gotStream, noStream);

function gotStream(stream) 
{
  if (window.URL) 
  {   camvideo.src = window.URL.createObjectURL(stream);   } 
  else // Opera
  {   camvideo.src = stream;   }

  camvideo.onerror = function(e) 
  {   stream.stop();   };

  stream.onended = noStream;
}

function noStream(e) 
{
  var msg = 'No camera available.';
  if (e.code == 1) 
  {   msg = 'User denied access to use camera.';   }
  document.getElementById('errorMessage').textContent = msg;
}
</script>

<!-- The code below contains a loop to draw the contents of the video tag
   onto the canvas tag, enabling us to do cool things with the image. -->
<!-- Based on http://www.adobe.com/devnet/html5/articles/javascript-motion-detection.html -->
<script>
// assign global variables to HTML elements
var video = document.getElementById( 'monitor' );
var videoCanvas = document.getElementById( 'videoCanvas' );
var videoContext = videoCanvas.getContext( '2d' );

var layer2Canvas = document.getElementById( 'layer2' );
var layer2Context = layer2Canvas.getContext( '2d' );

var blendCanvas  = document.getElementById( "blendCanvas" );
var blendContext = blendCanvas.getContext('2d');

var greenCanvas = document.getElementById( "greenCanvas");
var greenContext = greenCanvas.getContext('2d');

var messageArea = document.getElementById( "messageArea" );
var timeout
    
// background color if no video present
videoContext.fillStyle = '#005337';
videoContext.fillRect( 0, 0, videoCanvas.width, videoCanvas.height );       

var buttons = [];

var button1 = new Image();
button1.src ="images/nvr-motion detection.jpg";
var buttonData1 = { name:"MOTION DETECTED!!! =)", image:button1, x:150, y:100, w:75, h:75 };
buttons.push( buttonData1 );

var button2 = new Image();
button2.src ="images/ghost.jpeg";
var buttonData2 = { name:"You went through a ghost! =O", image:button2, x:375, y:300, w:75, h:75 };
buttons.push( buttonData2 );

// start the loop       
animate();

function animate() 
{
    requestAnimationFrame( animate ); //The window.requestAnimationFrame() method tells the browser that you wish to perform an animation and requests that the browser call a specified function to update an animation before the next repaint. The method takes as an argument a callback to be invoked before the repaint. 
  
  render(); //this renders the mirror video canvas and the canvas with buttons adding the video and buttons as graphics to their respective canvas. 
  blend();  
  checkAreas();
  // timeOut = setTimeout(animate(), 1000/60); //this sets how many pics/second are taken. wasn't working properly also, with the first parameter, was messing stuff up giving errors in console. Not necessary to have, was fun feature to add. 
}


// called in animate function
function render() 
{ 
  if ( video.readyState === video.HAVE_ENOUGH_DATA ) 
  {
    // mirror video
    
    videoContext.drawImage( video, 0, 0, videoCanvas.width, videoCanvas.height );
    for ( var i = 0; i < buttons.length; i++ )
      // this is where the buttons are being placed on one canvas.

      layer2Context.drawImage( buttons[i].image, buttons[i].x, buttons[i].y, buttons[i].w, buttons[i].h );
  }
}

var lastImageData;

function blend() 
{
  var width  = videoCanvas.width;
  var height = videoCanvas.height;
  // getImageData is getting pixel information for a certain canvas. params are (left, top, width, height)
  var sourceData = videoContext.getImageData(0, 0, width, height); // get current webcam image data
  if (!lastImageData) lastImageData = videoContext.getImageData(0, 0, width, height); // create an image if the previous image doesnâ€™t exist
  // create a ImageData instance to receive the blended result
  var blendedData = videoContext.createImageData(width, height);
  // blend the 2 images
  differenceAccuracy(blendedData.data, sourceData.data, lastImageData.data);
  
  // draw the result in a canvas
  blendContext.putImageData(blendedData, 0, 0);
  // store the current webcam image
  lastImageData = sourceData;
}

// this function is used in the blend function 
function differenceAccuracy(target, data1, data2) 
{
  if (data1.length != data2.length) return null; // if they're not the same width and height(canvas), can't compare accurate image pixel differences.
  var i = 0;
  while (i < (data1.length * 0.25)) 
  {
    var average1 = (data1[4*i] + data1[4*i+1] + data1[4*i+2]) / 3; 
    var average2 = (data2[4*i] + data2[4*i+1] + data2[4*i+2]) / 3;
    var diff = threshold(fastAbs(average1 - average2)); // this is comparing the difference in pixels between two canvas images. 
    target[4*i]   = diff;
    target[4*i+1] = diff;
    target[4*i+2] = diff;
    target[4*i+3] = 0xFF;
    ++i;
  }
}

// fastAbs and threshold functions are plugged into the differenceAccuracy function
function fastAbs(value) 
{
  return (value ^ (value >> 31)) - (value >> 31);
}
function threshold(value) // refers to the change in pixels, we are setting the threshold here for how big a change in pixels should be to want to detect a "motion change". FF might be referring to the white imaging when taking photos, when there is movement, it detects more white in the photo taking.
{
  return (value > 0x15) ? 0xFF : 0;
}
// check if white region from blend overlaps area of interest (e.g. buttons)
function checkAreas() 
{
  // buttons are the images being touched
  for (var b = 0; b < buttons.length; b++) 
  {
    // get the pixels in a note area from the blended image. blendContext is the black and white canvas, the one showing the white imagery for motion movement. 
    var blendedData = blendContext.getImageData( buttons[b].x, buttons[b].y, buttons[b].w, buttons[b].h );
      
    // calculate the average lightness of the blended data
    var i = 0;
    var sum = 0;
    var countPixels = blendedData.data.length * 0.25;
    while (i < countPixels) 
    {
      sum += (blendedData.data[i*4] + blendedData.data[i*4+1] + blendedData.data[i*4+2]);
      ++i;
    }
    // calculate an average between of the color values of the note area [0-255]
    var average = Math.round(sum / (3 * countPixels));
    if (average > 50) // more than 20% movement detected
    {
      
      console.log( "Button " + buttons[b].name + " triggered." ); // do stuff
      
      messageArea.innerHTML = "<font size='+10' color=" + buttons[b].name + "><b>Button " + buttons[b].name + " has been triggered.</b></font>";

      // if (messageArea.innerHTML === '<font size="+10" color="I" see="" you.....=""><b>Button I SEE YOU..... fucked.</b></font>') {
      //   console.log("POOP");
      //   layer2Context.drawImage( button4, 0, 0, 500, 500 );
      //   window.setTimeout(animate(), 5000);
      // }
    }
  }
}

</script>


</body>
</html>

<!-- resources: https://www.w3schools.com/tags/ref_canvas.asp
https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API
https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial/Using_images
https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Manipulating_video_using_canvas
http://www.adobe.com/devnet/archive/html5/articles/javascript-motion-detection.html
https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/getImageData
https://developer.mozilla.org/en-US/docs/Web/API/window/requestAnimationFrame
Sidenote: pixels range from 0 - 255, based on RGB, each having it's own pixel range[R][G][B]
 -->
